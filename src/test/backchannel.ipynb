{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd555b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackchannelIdentifier:\n",
    "    \"\"\"Two-stage backchannel detector.\"\"\"\n",
    "\n",
    "    # High-confidence anchored patterns for Stage 1\n",
    "    HIGH_CONFIDENCE_PATTERNS = [\n",
    "        r\"^yes$\", r\"^yeah$\", r\"^yup$\", r\"^ya$\", r\"^uh\\s*huh$\", r\"^uh$\", r\"^huh$\",\n",
    "        r\"^mhm$\", r\"^mm$\", r\"^m$\", r\"^um$\", r\"^umm$\", r\"^mm-hm$\",\n",
    "        r\"^oh$\", r\"^oh\\s+boy$\", r\"^oh\\s+yeah$\", r\"^cool$\", r\"^nice$\", r\"^right$\",\n",
    "        r\"^what\\??$\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, max_words: int = 3, max_length: int = 15, custom_patterns: Optional[list[str]] = None):\n",
    "        self.max_words = max_words\n",
    "        self.max_length = max_length\n",
    "        # Compile Stage 1 patterns\n",
    "        all_patterns = self.HIGH_CONFIDENCE_PATTERNS + (custom_patterns or [])\n",
    "        self.filter_pattern = re.compile(\"|\".join(all_patterns), re.IGNORECASE)\n",
    "\n",
    "    # ---------------- Stage 1 ----------------\n",
    "    def filter(self, target_utt: str) -> Optional[bool]:\n",
    "        \"\"\"Stage 1: high-confidence backchannel detection.\"\"\"\n",
    "        if not target_utt or isinstance(target_utt, float):\n",
    "            return False\n",
    "\n",
    "        utt = target_utt.strip()\n",
    "        word_count = len(utt.split())\n",
    "        char_count = len(utt)\n",
    "\n",
    "        # Length filter\n",
    "        if word_count > self.max_words or char_count > self.max_length:\n",
    "            return False  # too long to be a backchannel\n",
    "\n",
    "        # Anchored dictionary match\n",
    "        if self.filter_pattern.fullmatch(utt.lower()):\n",
    "            return True  # high-confidence backchannel\n",
    "\n",
    "        # Ambiguous / edge case\n",
    "        return None\n",
    "\n",
    "    # ---------------- judge ----------------\n",
    "    def judge(self, target_utt: str, previous_utt: str = \"\") -> bool:\n",
    "        \"\"\"Semantic judgment using context.\"\"\"\n",
    "        role_prompt = (\n",
    "        \"You are an expert in dialogue analysis. Your task is to determine whether the target utterance \"\n",
    "        \"is a backchannel behavior. Backchannel behaviors are brief listener responses that signal attention, \"\n",
    "        \"agreement, or understanding (e.g., 'uh-huh', 'yeah', 'right').\\n\"\n",
    "        \"Only respond with 'yes' if it is a backchannel, and 'no' otherwise. \"\n",
    "        \"Do not provide explanations.\\n\"\n",
    "        \"You will be given the previous turn as context. \"\n",
    "        \"The target utterance will always be labeled as <Target>.\"\n",
    "        )\n",
    "\n",
    "        task_prompt = f\"<Context> {self.previous_utt}, <Target>: {self.target_utt}\"\n",
    "\n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f\"{role_prompt} {task_prompt}\",\n",
    "        }]\n",
    "        response = ollama.chat(self.model_name, messages=messages)\n",
    "        message = response['message']\n",
    "        return message[\"content\"]\n",
    "\n",
    "    # ---------------- Combined method ----------------\n",
    "    def is_backchannel(self, target_utt: str, previous_utt: str = \"\") -> bool:\n",
    "        \"\"\"Full two-stage pipeline.\"\"\"\n",
    "        filter_result = self.filter(target_utt)\n",
    "        if filter_result is not None:\n",
    "            return filter_result  # high-confidence decision\n",
    "\n",
    "        # Stage 1 uncertain -> judge (LLM or semantic judge)\n",
    "        return self.judge(target_utt, previous_utt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2115e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import re\n",
    "\n",
    "class BackchannelIdentifier:\n",
    "    \"\"\"Two-stage backchannel detector with batch LLM judge.\"\"\"\n",
    "\n",
    "    HIGH_CONFIDENCE_PATTERNS = [\n",
    "        r\"^yes$\", r\"^yeah$\", r\"^yup$\", r\"^ya$\", r\"^uh\\s*huh$\", r\"^uh$\", r\"^huh$\",\n",
    "        r\"^mhm$\", r\"^mm$\", r\"^m$\", r\"^um$\", r\"^umm$\", r\"^mm-hm$\",\n",
    "        r\"^oh$\", r\"^oh\\s+boy$\", r\"^oh\\s+yeah$\", r\"^cool$\", r\"^nice$\", r\"^right$\",\n",
    "        r\"^what\\??$\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, max_words: int = 3, max_length: int = 15, custom_patterns: Optional[list[str]] = None):\n",
    "        self.max_words = max_words\n",
    "        self.max_length = max_length\n",
    "        all_patterns = self.HIGH_CONFIDENCE_PATTERNS + (custom_patterns or [])\n",
    "        self.filter_pattern = re.compile(\"|\".join(all_patterns), re.IGNORECASE)\n",
    "\n",
    "    # ---------------- Stage 1 ----------------\n",
    "    def filter(self, target_utt: str) -> Optional[bool]:\n",
    "        \"\"\"Stage 1: high-confidence backchannel detection.\"\"\"\n",
    "        if not target_utt or isinstance(target_utt, float):\n",
    "            return False\n",
    "\n",
    "        utt = target_utt.strip()\n",
    "        if len(utt.split()) > self.max_words or len(utt) > self.max_length:\n",
    "            return False  # too long to be a backchannel\n",
    "\n",
    "        if self.filter_pattern.fullmatch(utt.lower()):\n",
    "            return True  # high-confidence backchannel\n",
    "\n",
    "        return None  # ambiguous\n",
    "\n",
    "    # ---------------- Stage 2 ----------------\n",
    "    def judge_batch(self, previous_utts: pd.Series, target_utts: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Stage 2: semantic judgment using LLM in batch.\n",
    "        Returns a boolean Series aligned with input indices.\n",
    "        \"\"\"\n",
    "        role_prompt = (\n",
    "            \"You are an expert in dialogue analysis. Your task is to determine whether the target utterance \"\n",
    "            \"is a backchannel behavior. Backchannel behaviors are brief listener responses that signal attention, \"\n",
    "            \"agreement, or understanding (e.g., 'uh-huh', 'yeah', 'right').\\n\"\n",
    "            \"Only respond with 'yes' if it is a backchannel, and 'no' otherwise. \"\n",
    "            \"Do not provide explanations.\\n\"\n",
    "            \"The target utterance will always be labeled as <Target>.\"\n",
    "        )\n",
    "\n",
    "        # Build list of task prompts\n",
    "        task_prompts = [\n",
    "            f\"<Context> {prev}, <Target>: {targ}\"\n",
    "            for prev, targ in zip(previous_utts, target_utts)\n",
    "        ]\n",
    "\n",
    "        messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f\"{role_prompt} {task_prompts}\",\n",
    "        }]\n",
    "\n",
    "        response_list = ollama.chat(self.model_name, messages=messages)\n",
    "        # The order of response_list matches task_prompts\n",
    "\n",
    "        # For demonstration, fallback to 'no' for all\n",
    "        response_list = [\"no\"] * len(task_prompts)\n",
    "\n",
    "        # Convert 'yes'/'no' to boolean\n",
    "        llm_results = pd.Series([r.lower() == \"yes\" for r in response_list], index=previous_utts.index)\n",
    "        return llm_results\n",
    "\n",
    "    # ---------------- Full pipeline ----------------\n",
    "    def annotate_dataframe(self, df: pd.DataFrame, target_col='target_utt', previous_col='previous_utt', out_col='is_backchannel') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Annotate dataframe with backchannel detection.\n",
    "        Returns a new column with True/False.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        # Stage 1 filter\n",
    "        stage1_results = df[target_col].apply(self.filter)\n",
    "\n",
    "        # Identify ambiguous rows\n",
    "        ambiguous_mask = stage1_results.isna()\n",
    "        if ambiguous_mask.any():\n",
    "            # Stage 2 LLM judgment in batch\n",
    "            llm_results = self.judge_batch(\n",
    "                df.loc[ambiguous_mask, previous_col],\n",
    "                df.loc[ambiguous_mask, target_col]\n",
    "            )\n",
    "            # Fill back into stage1_results\n",
    "            stage1_results.loc[ambiguous_mask] = llm_results\n",
    "\n",
    "        # Ensure boolean type\n",
    "        df[out_col] = stage1_results.astype(bool)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2aa200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "model_name=\"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b02dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(self)->bool:\n",
    "    role_prompt = (\n",
    "        \"You are an expert in dialogue analysis. Your task is to determine whether the target utterance \"\n",
    "        \"is a backchannel behavior. Backchannel behaviors are brief listener responses that signal attention, \"\n",
    "        \"agreement, or understanding (e.g., 'uh-huh', 'yeah', 'right').\\n\"\n",
    "        \"Only respond with 'yes' if it is a backchannel, and 'no' otherwise. \"\n",
    "        \"Do not provide explanations.\\n\"\n",
    "        \"You will be given the previous turn as context. \"\n",
    "        \"The target utterance will always be labeled as <Target>.\"\n",
    "    )\n",
    "\n",
    "    task_prompt = f\"<Context> {self.previous_utt}, <Target>: {self.target_utt}\"\n",
    "\n",
    "    messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f\"{role_prompt} {task_prompt}\",\n",
    "    }]\n",
    "    response = ollama.chat(self.model_name, messages=messages)\n",
    "    message = response['message']\n",
    "    return message[\"content\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
